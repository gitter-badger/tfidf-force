{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Corpora\n",
    "\n",
    "Now that we have looked at analyzing and comparing documents, we can move to a higher unit of text. Sometime we want to look at a large collection of text in aggregate, such as the complete works of William Shakespeare, or all New York Times articles ever. The term we use for a collection of documents is corpus. And a corpus can be as large or as small as you want, but are usually collected together for some reason and have some meaning behind why they are grouped together. \n",
    "\n",
    "Lets look at a few examples we have direct access to through NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Preamble\n",
    "#\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import our core libraries\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can use the following to download corpora\n",
    "# nltk.download()\n",
    "\n",
    "# More info on the various corpora and corpus readers can be found here\n",
    "# http://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'austen-emma.txt',\n",
      " u'austen-persuasion.txt',\n",
      " u'austen-sense.txt',\n",
      " u'bible-kjv.txt',\n",
      " u'blake-poems.txt',\n",
      " u'bryant-stories.txt',\n",
      " u'burgess-busterbrown.txt',\n",
      " u'carroll-alice.txt',\n",
      " u'chesterton-ball.txt',\n",
      " u'chesterton-brown.txt',\n",
      " u'chesterton-thursday.txt',\n",
      " u'edgeworth-parents.txt',\n",
      " u'melville-moby_dick.txt',\n",
      " u'milton-paradise.txt',\n",
      " u'shakespeare-caesar.txt',\n",
      " u'shakespeare-hamlet.txt',\n",
      " u'shakespeare-macbeth.txt',\n",
      " u'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# Lets use a small sample of book from project gutenberg.\n",
    "pprint(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192427, 7344, 7752, u'austen-emma.txt')\n",
      "(98171, 5835, 3747, u'austen-persuasion.txt')\n",
      "(141576, 6403, 4999, u'austen-sense.txt')\n",
      "(1010654, 12767, 30103, u'bible-kjv.txt')\n",
      "(8354, 1535, 438, u'blake-poems.txt')\n",
      "(55563, 3940, 2863, u'bryant-stories.txt')\n",
      "(18963, 1559, 1054, u'burgess-busterbrown.txt')\n",
      "(34110, 2636, 1703, u'carroll-alice.txt')\n",
      "(96996, 8335, 4779, u'chesterton-ball.txt')\n",
      "(86063, 7794, 3806, u'chesterton-brown.txt')\n",
      "(69213, 6349, 3742, u'chesterton-thursday.txt')\n",
      "(210663, 8447, 10230, u'edgeworth-parents.txt')\n",
      "(260819, 17231, 10059, u'melville-moby_dick.txt')\n",
      "(96825, 9021, 1851, u'milton-paradise.txt')\n",
      "(25833, 3032, 2163, u'shakespeare-caesar.txt')\n",
      "(37360, 4716, 3106, u'shakespeare-hamlet.txt')\n",
      "(23140, 3464, 1907, u'shakespeare-macbeth.txt')\n",
      "(154883, 12452, 4250, u'whitman-leaves.txt')\n"
     ]
    }
   ],
   "source": [
    "# There are 18 documents of various lengths, \n",
    "# lets get some other basic info about them\n",
    "for fileid in gutenberg.fileids():\n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
    "    print(num_words, num_vocab, num_sents, fileid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity & The Vector Space Model\n",
    "\n",
    "One thing we may want to do with our documents is see which ones are similar to other ones. This would allow us to do things like automatically group 'similar' documents, which we will do via a technique called document clustering.\n",
    "\n",
    "A building block to automatically grouping them together (and a fun thing in its own right) is to try and see how similar one document is to another. How may we go about this? How can we __measure__ how similar one document is to another?\n",
    "\n",
    "One way of modelling documents to support this similarity calculation is the __vector space model of documents__. It starts by imagining the document as a _bag of words_ as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets turn the text of alice in wonderland into a bag of words with\n",
    "# associated frequency distribution.\n",
    "\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "alice = nltk.word_tokenize(alice)\n",
    "alice = [word.lower() for word in alice]\n",
    "frequencies = nltk.FreqDist(alice)\n",
    "print(frequencies.most_common(50))\n",
    "\n",
    "# FreqDist docs.\n",
    "# http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Vector Space Model\n",
    "\n",
    "Now if we imagine that each of the words represents a dimension in a high dimensional space. Then we each word-frequency pair represents a vector in this space, and further more the sum of those vectors gives a single vector that represents that entire document.\n",
    "\n",
    "Here is a diagram for what that may look like for a set of documents composed entirely of two words\n",
    "\n",
    "![Vector Space Model 2D](vector-space-2d-manning.png)\n",
    "*From \"Introduction to Information Retrieval\", Manning et al*\n",
    "\n",
    "In this model the frequency is known as the __term weight__ and provides the magnitude of the vector for that word. In the diagram above this has been normalzied to a score between 0 and 1. Term weight is a more general term because you can use things other than frequency as your term weight e.g. the TF-IDF score of a term.\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "Now that we have all the documents projected into a geometric space. How can we tell which documents are similar. \n",
    "\n",
    "One way would be to say vectors which are close to each other are similar, we could calculate the distance between two vectors and use that. However that can be problematic as documents of different lengths will have vastly different terms weights and thus skew the measure.\n",
    "\n",
    "Another approach would be to see that vectors lines that are going in the same-ish direction are going to be more similar. So if we can compare the directions they are going we can get a sense of this. One way to do this is to calculate the angle between two vectors, the smaller than angle the more similar their direction of travel and the more similar the documents. In practice is it easier to calculate the _cosine_ of the angle between the two vectors, this produces a nice number between 0 and 1 that we can use as our similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Letâ€™s create a vector space model out of some documents\n",
    "#and calculate some pairwise similarities.\n",
    "\n",
    "# Extract and normalize tokens for a given document.\n",
    "def get_tokens(fileid, corpus):\n",
    "    raw = corpus.raw(fileid)\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    norm = [token.lower() for token in tokens]\n",
    "    return norm\n",
    "\n",
    "# Takes all the tokens that appear in token_lists and puts them in a \n",
    "# set to determine unique tokens. Then creates a dictionary mapping a token\n",
    "# to its index in the set, this enables us to have a unique target position for\n",
    "# each word in our vocabulary.\n",
    "def build_vocabulary(token_lists):\n",
    "    result = set()\n",
    "    for tl in token_lists:\n",
    "        result = result.union(set(tl))\n",
    "    result = {v:i for i,v in enumerate(result)}\n",
    "    return result\n",
    "\n",
    "# Builds a vector for a given token list and vocabulary.\n",
    "# The result is a vector with length equal to the number of words in the\n",
    "# vocabulary, and term weights for each token in the token list set in \n",
    "# the appropriate position\n",
    "def build_vector(tokens, vocabulary):\n",
    "    result = [0] * len(vocabulary)\n",
    "    freq = Counter(tokens)\n",
    "    for token in tokens:\n",
    "        pos = vocabulary[token]\n",
    "        result[pos] = freq[token]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice = get_tokens('carroll-alice.txt', gutenberg)\n",
    "moby = get_tokens('melville-moby_dick.txt', gutenberg)\n",
    "austen1 = get_tokens('austen-emma.txt', gutenberg)\n",
    "austen2 = get_tokens('austen-persuasion.txt', gutenberg)\n",
    "austen3 = get_tokens('austen-sense.txt', gutenberg)\n",
    "\n",
    "vocabulary = build_vocabulary([alice, moby, austen1, austen2, austen3]);\n",
    "print(len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice_v = build_vector(alice, vocabulary)\n",
    "moby_v = build_vector(moby, vocabulary)\n",
    "austen1_v = build_vector(austen1, vocabulary)\n",
    "austen2_v = build_vector(austen2, vocabulary)\n",
    "austen3_v = build_vector(austen3, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(alice_v[0:50])\n",
    "print(\"\\n\")\n",
    "print(list(vocabulary)[0:50])\n",
    "print(\"\\n\")\n",
    "print(zip(alice_v[0:50], list(vocabulary)[0:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets compare Alice in Wonderland to the Others\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "all_vectors = [alice_v, moby_v, austen1_v, austen2_v, austen3_v]\n",
    "for v in all_vectors:\n",
    "    co = cosine(alice_v, v)\n",
    "    print(1 - co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets compare Austen1 to the others\n",
    "for v in all_vectors:\n",
    "    co = cosine(austen1_v, v)\n",
    "    print(1 - co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we use more functionality from SciPy we can also see this all in one matrix\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "all_vectors_np = np.array(all_vectors)\n",
    "similary = 1 - pairwise_distances(all_vectors_np, metric=\"cosine\")\n",
    "print(similary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Documents\n",
    "\n",
    "Now that we can determine which documents are similar to others, we can take advantage of a technique known as Clustering. Clustering is a machine learning technique that tries to automatically group items into a set of K sub groups (where you provide the number K). It can be used to see if there are groups that emerge out of the properties of the documents in the larger collection. Lets try clustering these documents and see if they cluster by author.\n",
    "\n",
    "## K-Means Clustering. \n",
    "\n",
    "The clustering algorithm we will look at here is K-Means. There are others but K-means is relatively simple to intuitively understand. \n",
    "\n",
    "It works by first randomly generating some cluster centers and assigning elements to these clusters by proximity. The cluster centers are then iterateively moved to the centroid of the clustered points and the process repeated.\n",
    "\n",
    "![Vector Space Model 2D](k-means-ng-jordan.png)\n",
    "\n",
    "> From Stanford CS229 Lecture Notes by Andrew Ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets see this in action\n",
    "# http://www.nltk.org/api/nltk.cluster.html\n",
    "# http://www.nltk.org/_modules/nltk/cluster/kmeans.html\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from numpy import array\n",
    "\n",
    "num_clusters = 3\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=5)\n",
    "\n",
    "vectors = [array(f) for f in all_vectors] \n",
    "clusters = kclusterer.cluster(vectors, True) \n",
    "print('Clustering results:', clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets do this for all 18 documents\n",
    "\n",
    "doc_ids = gutenberg.fileids()\n",
    "token_lists = [get_tokens(f, gutenberg) for f in doc_ids]\n",
    "voc = build_vocabulary(token_lists)\n",
    "doc_vectors = [array(build_vector(tl, voc)) for tl in token_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_clusters = 12 #note there are 12 authors\n",
    "kclusterer = KMeansClusterer(num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=10)\n",
    "clusters = kclusterer.cluster(doc_vectors, True) \n",
    "\n",
    "doc_clusters = zip(gutenberg.fileids(), clusters)\n",
    "for dc in doc_clusters:\n",
    "    pprint(dc)\n",
    "    \n",
    "# Note that running this multiple times produces different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clusters)\n",
    "kclusterer.means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MDS to plot high dimensional vectors.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "plt.figure(figsize=(10,10)) \n",
    "\n",
    "mds = MDS(n_components=2, random_state=1)\n",
    "pos = mds.fit_transform(doc_vectors)\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "for x, y, name, cluster in zip(xs, ys, gutenberg.fileids(), clusters):\n",
    "    color = 'blue'\n",
    "    plt.scatter(x, y, c=color)\n",
    "    plt.text(x, y, name + \" \" + str(cluster))\n",
    "    \n",
    "cluster_pos = mds.fit_transform(kclusterer.means())\n",
    "xs, ys = cluster_pos[:, 0], pos[:, 1]\n",
    "\n",
    "#for x, y, name in zip(xs, ys, range(num_clusters)):\n",
    "#    color = 'orange'\n",
    "#    plt.scatter(x, y, c=color)\n",
    "#    plt.text(x, y, name)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Considerations when clustering\n",
    "\n",
    "There are a number of things to keep in mind when clustering\n",
    "\n",
    "* Feature Selection: How you select your features (and thus build your document vectors) is very important. Feel free to experiment with different terms weights and also differing features (e.g. removing stopwords, or stemming words to their roots). Anything you can count systematically across your documents can be a feature!\n",
    "* Choise of algorithm: There are many different clustering algorithms and we have only looked at two. While simple sounding, K-Means is actually widely used in production because it is relatively fast and gives pretty good results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Cluster the speeches in the inaugural collection, see if you can recover author distribution or maybe time/era distributions.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exercise 1.\n",
    "\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from numpy import array\n",
    "\n",
    "# Extract and normalize tokens for a given document.\n",
    "def get_tokens(fileid, corpus):\n",
    "    raw = corpus.raw(fileid)\n",
    "    tokens = nltk.word_tokenize(raw)\n",
    "    norm = [token.lower() for token in tokens]\n",
    "    return norm\n",
    "\n",
    "# Takes all the tokens that appear in token_lists and puts them in a \n",
    "# set to determine unique tokens. Then creates a dictionary mapping a token\n",
    "# to its index in the set, this enables us to have a unique target position for\n",
    "# each word in our vocabulary.\n",
    "def build_vocabulary(token_lists):\n",
    "    result = set()\n",
    "    for tl in token_lists:\n",
    "        result = result.union(set(tl))\n",
    "    result = {v:i for i,v in enumerate(result)}\n",
    "    return result\n",
    "\n",
    "# Builds a vector for a given token list and vocabulary.\n",
    "# The result is a vector with length equal to the number of words in the\n",
    "# vocabulary, and term weights for each token in the token list set in \n",
    "# the appropriate position\n",
    "def build_vector(tokens, vocabulary):\n",
    "    result = [0] * len(vocabulary)\n",
    "    freq = Counter(tokens)\n",
    "    for token in tokens:\n",
    "        pos = vocabulary[token]\n",
    "        result[pos] = freq[token]\n",
    "    return result\n",
    "\n",
    "# The ids for the speeches. Print these out if you want to get a sense of\n",
    "# what is in this corpus.\n",
    "speech_names = inaugural.fileids()\n",
    "\n",
    "\n",
    "# Step 1. Tokenize the speeches\n",
    "\n",
    "# Step 2. Build a vocabulary for all the speeches this is the\n",
    "# list of all the features (all the terms) across the entire corpus\n",
    "\n",
    "# Step 3. Build feature vectors for the individual speeches.  \n",
    "\n",
    "\n",
    "# Step 4. Do the clustering. Feel free to pick between K-Means and GAAC. \n",
    "# How many clusters might you want to generate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "* [Programming Collective Intelligence by Tony Segaran](http://shop.oreilly.com/product/9780596529321.do?sortby=publicationDate) â€” A great lightweight and hands on introduction to various machine learning concepts such as clustering and classification.\n",
    "\n",
    "* [Introduction to Information Retreival by Manning et Al](http://nlp.stanford.edu/IR-book/) â€” Online textbook covering a number of foundational topics in information retreival and statistical text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
